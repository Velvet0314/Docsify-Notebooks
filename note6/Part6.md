# 第六章&ensp;支持向量机

在这一章节中我们将进行学习 **支持向量机（support vector machine）**。支持向量机一度被许多人认为是最好的 “即用式” 监督学习算法。通俗来讲，SVM 是一种二分类模型。接下来，我们将深入 SVM，来看看其是否是如此的优秀。

通过下面的学习，应该对以下知识有着基本的了解：

* 拉格朗日对偶
* 正则化

通过下面的学习，应该重点掌握：

* SVM 的定义
* 函数边界与几何边界
* 最优边界决策器
* 核 Kernel
* SMO 算法

- - -

#### 边界：直观感受

让我们回到最开始的二分类问题。当时我们采用的解决方式是逻辑回归。

考虑逻辑回归，其中概率 $ p(y = 1|x; \theta) $ 由 $ h_\theta(x) = g(\theta^T x) $ 建模。

如果 $ h_\theta(x) \geq 0.5 $，或等效地，如果 $ \theta^T x \geq 0 $，我们就会在输入 $ x $ 上预测 1。考虑一个正样本（$ y = 1 $），$ \theta^T x $ 越大，$ h_\theta(x) = p(y = 1|x; w, b) $ 也越大，因此我们对标签是 1 的 “置信度” 也越高。

我们可以认为如果 $ \theta^T x \gg 0 $，我们的预测非常确定 $ y = 1 $。类似地，我们认为逻辑回归在 $ \theta^T x \ll 0 $ 时对 $ y = 0 $ 做出高置信度的预测。考虑到训练集，我们似乎找到了一个很好的拟合训练数据的模型：

**如果我们能找到使 $ \theta^T x^{(i)} \gg 0 $ 的 $ \theta $ 确信 $ y^{(i)} = 1 $，且 $ \theta^T x^{(i)} \ll 0 $ 确信 $ y^{(i)} = 0 $。**

为了更直观地感受 **边界（margin）**，样例如下图所示。

其中 $ × $ 表示正训练样本，$ \text{o} $ 表示负训练样本，决策边界是由方程 $ \theta^T x = 0 $ 给出的直线，也称为 <strong>分隔超平面（separating hyperplane）</strong>

<div style="text-align: center;">
 <a href="https://s21.ax1x.com/2024/07/08/pkfPHRP.png" data-lightbox="image-6" data-title="margin">
  <img src="https://s21.ax1x.com/2024/07/08/pkfPHRP.png" alt="margin" style="width:100%;max-width:375px;cursor:pointer">
 </a>
</div>

请注意，点 A 离决策边界很远。如果我们被要求在 A 处对 $ y $ 的值进行预测，似乎我们应该确信 $ y = 1 $。相反，点 C 非常靠近决策边界，虽然它位于我们会预测 $ y = 1 $ 的决策边界的一侧，但似乎只需对决策边界稍作改动，就很容易使我们的预测变为 $ y = 0 $。因此，我们对 A 的预测比对 C 的预测置信度更高，而点 B 位于这两种情况之间。

进一步地，如果一个点离分隔超平面很远，那么其可能在我们的预测中更置信。也就是说，训练样本需要离边界尽可能地远。这样会使得我们的模型更加有效。

#### SVM 定义

为了易于后续学习 SVM，我们首先需要引入一些新的符号来讨论分类问题。

我们将考虑一个用于二元分类问题的线性分类器，其标签为 $ y $ 并且特征为 $ x $。从现在开始，我们将使用 $ y \in \\{-1, 1\\} $ 来表示类标签。此外，我们不再用向量 $ \theta $ 来描述我们的线性分类器，而是使用参数 $ w, b $。于是将分类器写为：

<div class="math">
$$
h_{w,b}(x) = g(w^T x + b)
$$
</div>

如果 $ z \geq 0 $，那么有 $ g(z) = 1 $；否则 $ g(z) = -1 $。

这种 $ w, b $ 符号让我们可以更明确地单独处理截距项 $ b $ 和其他参数。同时，也放弃了之前让 $ x_0 = 1 $ 作为输入特征向量中的一个额外项。在这里，$ b $ 取代了 $ \theta_0 $，而 $ w $ 取代了 $[ \theta_1 \ldots \theta_n ]^T$。

> [!WARNING]
> **根据我们上面定义的 $ g $，我们的分类器将直接预测 1 或 −1（参考感知机算法），而不是首先估计 $ y = 1 $ 的概率（这是逻辑回归所做的）。**

> [!TIP]
> 逻辑回归与支持向量机的定义区别<br>
> <table>
>     <thead>
>         <tr>
>             <th>特性</th>
>             <th>逻辑回归</th>
>             <th>支持向量机</th>
>         </tr>
>     </thead>
>     <tbody>
>         <tr>
>             <td><strong>标签和输出</strong></td>
>             <td>输出标签为 {0, 1}，输出为概率</td>
>             <td>输出标签为 {-1, 1}，直接预测类别</td>
>         </tr>
>         <tr>
>             <td><strong>决策函数</strong></td>
>             <td>使用 sigmoid 函数预测概率，连续</td>
>             <td>使用符号函数（感知机），离散</td>
>         </tr>
>     </tbody>
> </table>

#### 边界

##### 函数边界

对于一个给定的训练样本 $ \\{x^{(i)},y^{(i)}\\} $，定义关于 $ (w,b) $ 关于样本的 **函数边界（functional margin）**为：

<div class="math">
$$
\hat{\gamma}^{(i)} = y^{(i)}(w^T x + b)
$$
</div>

对于支持向量机，其决策分数通常表现为 $ w^T x + b $：
- 当 $ w^T x + b = 0 $ 时，点恰好在决策边界上，即函数边界。
- 当 $ w^T x + b > 0 $ 时，模型预测样本属于正类（$ y = 1 $）。
- 当 $ w^T x + b < 0 $ 时，模型预测样本属于负类（$ y = -1 $）。

通过之前的学习，我们得出结论：要使结果置信度更高，那么就要求 $ \left| w^T x + b \right| $ 需要尽可能的大。

考虑两种情况，当 $ y = 1 $ 时，有 $ w^T x + b \gg 0 $；当 $ y = -1 $ 时，有 $ w^T x + b \ll 0 $。

但是针对于函数边界，存在一个特殊的问题。

如果我们用 $ 2w $ 和 $ 2b $ 替换 $w$ 和 $b$ （进行放缩），就会得到 $ g(w^T x + b) = g(2w^T x + 2b) $，这不会改变 $ h_{w,b}(x) $ 预测结果的正负。然而，用 $(2w, 2b)$ 替换 $(w, b)$ 却会使函数边界增加 2 倍。也就是说，通过任意的对于 $w$ 和 $b$ 的放缩，使得函数边界在衡量置信度上失去了意义（但实际的决策平面未改变）。为了解决这个问题，我们希望能够添加某种规范化条件，比如 $\|\|w\|\|_2 = 1$。也就是说，我们可能会用 $(\displaystyle{\frac{w}{\|\|w\|\|_2}}, \displaystyle{\frac{b}{\|\|w\|\|_2}})$ 替换 $(w, b)$，并相应地考虑 $(\displaystyle{\frac{w}{\|\|w\|\|_2}}, \displaystyle{\frac{b}{\|\|w\|\|_2}})$ 的函数边界。

> [!NOTE]
> **分隔平面 $ w^T x + b = 0 $ 描述的是一个通过原点的超平面，其方向由向量 $ w $ 确定，偏移由 $ b $ 确定。当 $ w $ 和 $ b $ 同时乘以同一个非零常数时，超平面的方向和偏移比例保持不变，因此超平面本身也保持不变。**

给定训练集 $ S = \\{(x^{(i)}, y^{(i)}) ; i = 1, \dots, m\\} $，我们还定义了 $ (w, b) $ 相对于 $ S $ 的函数边界为最小的那个个体训练样本的函数边界。通过 $ \hat{\gamma} $ 表示，写为：

<div class="math">
$$
\hat{\gamma} = \min_{i=1,...,m} \hat{\gamma}^{(i)}
$$
</div>

##### 几何边界

首先从下面这张图开始：

<div style="text-align: center;">
 <a href="https://s21.ax1x.com/2024/07/09/pkf3wU1.png" data-lightbox="image-6" data-title="geometric margin">
  <img src="https://s21.ax1x.com/2024/07/09/pkf3wU1.png" alt="geometric margin" style="width:100%;max-width:425px;cursor:pointer">
 </a>
</div>

决策边界对应于 $ (w, b) $ 如图所示，同时还标记了向量 $ \vec{w} $。注意 $ \vec{w} $ 与超平面是正交的。也就是说，$ \vec{w} $ 表示了样本到超平面的一个法向量。

考虑点 $ A $，它代表某个训练样本的输入 $ x^{(0)} $ 与标签 $ y^{(0)} = 1 $。其到决策边界的距离 $ \gamma^{(0)} $ 由线段 $AB$ 给出。

如何确定 $\gamma^{(0)} $ 的值呢？已知 $ \displaystyle{\frac{\vec{w}}{\|\vec{w}\|}} $ 是一个指向 $ \vec{w} $ 同一方向的单位向量。由点 $ A $ 表示 $ x^{(0)} $，得点 $ B $ 为 $ x^{(0)} - \gamma^{(0)} \cdot (\displaystyle{\frac{\vec{w}}{\|\vec{w}\|}}) $。

> [!NOTE]
> 由于点 $ B $ 在决策边界上，从 $ A $ 到 $ B $ 的向量（$ \vec{AB} $）必须与 $ \vec{w} $ 方向一致，并且是垂直于边界的。因此，$ \vec{AB} $ 可以表达为 $ \vec{w} $ 单位向量的某个标量倍数，即：
>
> <div class="math">
> $$ 
> \vec{AB} = -\gamma^{(0)} \cdot \left(\frac{\vec{w}}{\|\vec{w}\|}\right)
> $$
> </div>
> 
> 这里使用负号，因为我们从 $ A $ 沿 $ \vec{w} $ 的方向“移动”到边界，若 $ \vec{w} $ 从 $ A $ 指向超平面外，则移动方向为 $ \vec{w} $ 的反方向。
>点 $ B $ 的位置 $ x^{(B)} $ 由下式给出：
>
> <div class="math">
> $$ 
> x^{(B)} = x^{(0)} + \vec{AB} = x^{(0)} - \gamma^{(0)} \cdot \left(\frac{\vec{w}}{\|\vec{w}\|}\right)
> $$
> </div>

考虑一般情况，决策边界上的所有点都满足方程 $ w^T x + b = 0 $。因此，有：

<div class="math">
$$
w^T \left(x^{(i)} - \gamma^{(i)} \cdot \frac{w}{\|w\|}\right) + b = 0
$$
</div>

求解 $ \gamma^{(i)} $ 得：

<div class="math">
$$
\gamma^{(i)} = \frac{w^T x^{(i)} + b}{\|w\|} = \left(\frac{w}{\|w\|}\right)^T x^{(i)} + \frac{b}{\|w\|}
$$
</div>

这是针对 $ A $ 在图中所处 "正样本" 位置的情况计算得出。更普遍地，我们定义一个训练样本 $ \\{x^{(i)},y^{(i)}\\} $ 相对于 $ (w, b) $ 的 **几何边界（geometric margin）** 为：

<div class="math">
$$
\gamma^{(i)} = y^{(i)} \left( \left(\frac{w}{\|w\|}\right)^T x^{(i)} + \frac{b}{\|w\|} \right)
$$
</div>

注意，如果 $ \|w\| = 1 $，那么函数边界就等于几何边界。同时，几何边界具有放缩不变性。如果我们用 $ 2w $ 和 $ 2b $ 替换 $ w $ 和 $ b $，几何边界不会改变。因为经过前面的推导，几何边界实际上是样本点到超平面的欧几里得距离。

几何边界所具有放缩不变性会为 SVM 带来许多优势：

- **模型简化与正则化**：
调整权重向量 $ w $ 和偏置 $ b $ 的尺度不会影响模型的决策边界。这允许在模型训练过程中引入正则化约束，如限制 $ \|w\| $ 的大小，从而防止过拟合。
- **参数选择的灵活性**：
在部署模型到不同硬件或软件环境时，可能需要对模型参数进行缩放以满足特定的性能或资源使用要求，缩放不变性确保了这种调整不会影响模型的表现。

最后，给定一个训练集 $ S = \\{(x^{(0)},y^{(0)}); i = 1, \ldots, m\\} $，我们也定义了 $ (w, b) $ 相对于 $ S $ 的几何边界为个体训练样本几何边界的最小值：

<div class="math">
$$
\gamma = \min_{i=1,\ldots,m} \gamma^{(i)}
$$
</div>

> [!TIP]
> **<font size=4>函数边界与几何边界的联系</font>**<br>
> **函数边界：**
> <div class="math">
> $$
> \hat{\gamma}^{(i)} = y^{(i)}(w^T x + b)
> $$
> </div>
> 
> **几何边界：**
> <div class="math">
> $$
> \gamma^{(i)} = \frac{y^{(i)}(w^T x + b)}{|| w ||}
> $$
> </div>
> 
> **当 $ || w || = 1 $ 时，函数边界与几何边界相等。**

#### 拉格朗日对偶

#### 最优边界决策器

##### 深入：最优边界决策器

#### 核 Kernel

#### 正则化

#### SMO 算法
